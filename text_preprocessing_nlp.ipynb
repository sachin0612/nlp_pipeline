{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed676072",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f697c25",
   "metadata": {},
   "source": [
    "# 1. Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91b9459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a9c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b959cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dac82958",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7587f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. <br /><br />the...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f40660",
   "metadata": {},
   "source": [
    "# 2. Remove HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c67fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eea697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a183938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. the filming tec...  positive"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a055e",
   "metadata": {},
   "source": [
    "# 3. Removing URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b400f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Check out my new notebook on https://www.colab.com/csv/notebook161745\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97ab5645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4b74db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check out my new notebook on '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd462362",
   "metadata": {},
   "source": [
    "# 4. Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2e2bae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time, string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78d830f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40d3f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text = text.replace(char,'')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1409234",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'string. with. Punctuation!?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa86967a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check out my new notebook on httpswwwcolabcomcsvnotebook161745'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punc(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4632021a",
   "metadata": {},
   "source": [
    "# Another Method to remove punctuation with fast speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e59dff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    return text.translate(str.maketrans('','',exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7abb2620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'string with Punctuation'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punc(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40af712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c49b0aa",
   "metadata": {},
   "source": [
    "# 5. Chat word treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf6893c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chats_dict = {\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"ATK\": \"At The Keyboard\",\n",
    "    \"ATM\": \"At The Moment\",\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"BAK\": \"Back At Keyboard\",\n",
    "    \"BBL\": \"Be Back Later\",\n",
    "    \"BBS\": \"Be Back Soon\",\n",
    "    \"BFN\": \"Bye For Now\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BRT\": \"Be Right There\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"B4\": \"Before\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"CU\": \"See You\",\n",
    "    \"CUL8R\": \"See You Later\",\n",
    "    \"CYA\": \"See You\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"FC\": \"Fingers Crossed\",\n",
    "    \"FWIW\": \"For What It's Worth\",\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"GAL\": \"Get A Life\",\n",
    "    \"GG\": \"Good Game\",\n",
    "    \"GN\": \"Good Night\",\n",
    "    \"GMTA\": \"Great Minds Think Alike\",\n",
    "    \"GR8\": \"Great!\",\n",
    "    \"G9\": \"Genius\",\n",
    "    \"IC\": \"I See\",\n",
    "    \"ICQ\": \"I Seek you (also a chat program)\",\n",
    "    \"ILU\": \"I Love You\",\n",
    "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"IOW\": \"In Other Words\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    \"KISS\": \"Keep It Simple, Stupid\",\n",
    "    \"LDR\": \"Long Distance Relationship\",\n",
    "    \"LMAO\": \"Laugh My A.. Off\",\n",
    "    \"LOL\": \"Laughing Out Loud\",\n",
    "    \"LTNS\": \"Long Time No See\",\n",
    "    \"L8R\": \"Later\",\n",
    "    \"MTE\": \"My Thoughts Exactly\",\n",
    "    \"M8\": \"Mate\",\n",
    "    \"NRN\": \"No Reply Necessary\",\n",
    "    \"OIC\": \"Oh I See\",\n",
    "    \"PITA\": \"Pain In The A..\",\n",
    "    \"PRT\": \"Party\",\n",
    "    \"PRW\": \"Parents Are Watching\",\n",
    "    \"QPSA\": \"Que Pasa?\",\n",
    "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
    "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
    "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n",
    "    \"SK8\": \"Skate\",\n",
    "    \"STATS\": \"Your sex and age\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"THX\": \"Thank You\",\n",
    "    \"TTFN\": \"Ta-Ta For Now!\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"U\": \"You\",\n",
    "    \"U2\": \"You Too\",\n",
    "    \"U4E\": \"Yours For Ever\",\n",
    "    \"WB\": \"Welcome Back\",\n",
    "    \"WTF\": \"What The F...\",\n",
    "    \"WTG\": \"Way To Go!\",\n",
    "    \"WUF\": \"Where Are You From?\",\n",
    "    \"W8\": \"Wait...\",\n",
    "    \"7K\": \"Sick:-D Laugher\",\n",
    "    \"TFW\": \"That feeling when\",\n",
    "    \"MFW\": \"My face when\",\n",
    "    \"MRW\": \"My reaction when\",\n",
    "    \"IFYP\": \"I feel your pain\",\n",
    "    \"LOL\": \"Laughing out loud\",\n",
    "    \"TNTL\": \"Trying not to laugh\",\n",
    "    \"JK\": \"Just kidding\",\n",
    "    \"IDC\": \"I don’t care\",\n",
    "    \"ILY\": \"I love you\",\n",
    "    \"IMU\": \"I miss you\",\n",
    "    \"ADIH\": \"Another day in hell\",\n",
    "    \"ZZZ\": \"Sleeping, bored, tired\",\n",
    "    \"WYWH\": \"Wish you were here\",\n",
    "    \"TIME\": \"Tears in my eyes\",\n",
    "    \"BAE\": \"Before anyone else\",\n",
    "    \"FIMH\": \"Forever in my heart\",\n",
    "    \"BSAAW\": \"Big smile and a wink\",\n",
    "    \"BWL\": \"Bursting with laughter\",\n",
    "    \"LMAO\": \"Laughing my a** off\",\n",
    "    \"BFF\": \"Best friends forever\",\n",
    "    \"CSL\": \"Can’t stop laughing\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "543d02a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text=[]\n",
    "    for w in text.split():\n",
    "        if w.upper() in chats_dict:\n",
    "            new_text.append(chats_dict[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "429cb2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can’t stop laughing on Ramesh'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversion('CSL on Ramesh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ce93e",
   "metadata": {},
   "source": [
    "# 6. Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "468dce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dcc4943",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_text = 'ceertain coonditon dduuring seveal ggenerattion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61ebca16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'certain condition during several generation'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textBlb = TextBlob(incorrect_text)\n",
    "textBlb.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbc6c93",
   "metadata": {},
   "source": [
    "# 7. Removing Stop words"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa41635d",
   "metadata": {},
   "source": [
    "Stop words are common words in a language that are often filtered out during natural language processing tasks, such as \n",
    "text analysis and information retrieval. These words typically do not carry significant meaning or contribute to the \n",
    "understanding of the text. Examples of stop words in English include \"the,\" \"is,\" \"at,\" \"on,\" \"in,\" and \"of.\" Removing stop \n",
    "words can help improve the efficiency and accuracy of text processing tasks by reducing the amount of data to be analyzed and \n",
    "focusing on the more meaningful content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9890f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac20b0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58f341f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "593c6fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probably  all-time favourite movie,  story  selflessness  dedication   noble cause,'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords('probably my all-time favourite movie, a story of selflessness and dedication to a noble cause,')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0756fe",
   "metadata": {},
   "source": [
    "# 8. Handling Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea59e7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I am on🔥'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8197d9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am on:fire:\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "print(emoji.demojize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba167ac",
   "metadata": {},
   "source": [
    "# 9. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6b4a8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'bengaluru.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenization\n",
    "s1 = 'I am going to bengaluru.'\n",
    "s1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8272e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to bengaluru', ' I am eating mango', '']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "s2 = 'I am going to bengaluru. I am eating mango.'\n",
    "s2.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6e8dd",
   "metadata": {},
   "source": [
    "# Challenges in tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2e3edd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where do think I should go? I have 3 days holiday']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3 = 'Where do think I should go? I have 3 days holiday'\n",
    "s3.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6081ad47",
   "metadata": {},
   "source": [
    "# Using NLTK library to do tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "329d485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92f0c76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'visit', 'delhi', '!']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"I am going to visit delhi!\"\n",
    "word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a51f85dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed consequat felis eget justo vestibulum, sit amet lacinia turpis venenatis. Nunc vitae justo nec nisi sagittis blandit. Nullam at ex in justo cursus condimentum. Aliquam erat volutpat. Nulla facilisi. Proin accumsan sem sit amet varius mattis.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2e290bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem ipsum dolor sit amet, consectetur adipiscing elit.',\n",
       " 'Sed consequat felis eget justo vestibulum, sit amet lacinia turpis venenatis.',\n",
       " 'Nunc vitae justo nec nisi sagittis blandit.',\n",
       " 'Nullam at ex in justo cursus condimentum.',\n",
       " 'Aliquam erat volutpat.',\n",
       " 'Nulla facilisi.',\n",
       " 'Proin accumsan sem sit amet varius mattis.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9de18e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = 'I have a Ph.D. in A.I.'\n",
    "s2 = \"we're here to help! mail us at abc@gmail.com\"\n",
    "s3 = 'A 5km ride cost $10.50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0dde2fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'a', 'Ph.D.', 'in', 'A.I', '.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "664889b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " \"'re\",\n",
       " 'here',\n",
       " 'to',\n",
       " 'help',\n",
       " '!',\n",
       " 'mail',\n",
       " 'us',\n",
       " 'at',\n",
       " 'abc',\n",
       " '@',\n",
       " 'gmail.com']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "da3f2d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', '5km', 'ride', 'cost', '$', '10.50']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4caddb",
   "metadata": {},
   "source": [
    "# Using spacy library to do tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1814067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceab83a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(s1)\n",
    "doc2 = nlp(s2)\n",
    "doc3 = nlp(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112774f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc1:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce5bdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05459828",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc3:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6bf129",
   "metadata": {},
   "source": [
    "# 10. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c69caf",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing words to their root or base form, even if the root itself may not be a valid word. \n",
    "This is typically achieved by removing suffixes or prefixes from words, allowing variations of a word to be treated as a \n",
    "single entity. For example, stemming would convert \"running,\" \"ran,\" and \"runner\" to the common root \"run.\" It's commonly \n",
    "used in natural language processing and text mining to simplify text analysis by treating different forms of the same word \n",
    "as equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba23267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful in Information Retriever system like google search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f5ad0f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9839ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e60f8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"walk walks walked walking\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5869cbab",
   "metadata": {},
   "source": [
    "# 11. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731e53c8",
   "metadata": {},
   "source": [
    "Lemmatization is the process of reducing words to their base or dictionary form, known as the lemma. Unlike stemming, \n",
    "which simply chops off prefixes or suffixes, lemmatization considers the meaning of the word and applies morphological \n",
    "analysis to transform it into its canonical form. This ensures that the resulting lemma is a valid word. For example, the \n",
    "lemma of \"was\" is \"be,\" and the lemma of \"mice\" is \"mouse.\" Lemmatization is often preferred over stemming for tasks where \n",
    "the meaning of words is important, such as in natural language understanding and information retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801ea9d6",
   "metadata": {},
   "source": [
    "Consider the word \"better.\"\n",
    "\n",
    "Stemming might simply remove suffixes to get the root word, so \"better\" might become \"bet.\"\n",
    "\n",
    "Lemmatization, however, considers the meaning of the word and reduces it to its base form or lemma. In this case, \"better\" \n",
    "remains \"better\" because it's already in its base form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865a9eea",
   "metadata": {},
   "source": [
    "Stemming is generally faster than lemmatization because it's a simpler process. Stemming typically involves removing \n",
    "common prefixes and suffixes from words to get to their root form, while lemmatization requires more complex morphological \n",
    "analysis to return the base or dictionary form of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "544db407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized words: ['running', 'ate', 'beautiful', 'cat']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example words\n",
    "words = [\"running\", \"ate\", \"beautiful\", \"cats\"]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(\"Lemmatized words:\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20ed8150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized words: ['run', 'eat', 'beautiful', 'cat']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example words\n",
    "words = [\"running\", \"ate\", \"beautiful\", \"cats\"]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words] # lemmatization based on verb\n",
    "print(\"Lemmatized words:\", lemmatized_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
